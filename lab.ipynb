{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDTRAIA_API - Research Project\\nJupyter Lab for testing the model and the LoRA Adapter, in case.\\nThis is the pre-implementation of the logic in main.py\\nUse this if you dont want to start the server or for testing purposes\\nAuthors: Rodrigo Alvarez, Adrian Rodriguez, Uriel Perez\\nCreated on: 2023 \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DTRAIA_API - Research Project\n",
    "Jupyter Lab for testing the model and the LoRA Adapter, in case.\n",
    "This is the pre-implementation of the logic in main.py\n",
    "Use this if you dont want to start the server or for testing purposes\n",
    "Authors: Rodrigo Alvarez, Adrian Rodriguez, Uriel Perez\n",
    "Created on: 2023 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-13 23:17:53,575] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "from dtraia_llm.models.llama2 import Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_ADAPTER = \"Rodr16020/GNS3_Python_Code_Llama-2-Chat-LoRA-Seele-v_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed439131307d40e7919f838fa7e2ad19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_2 = Llama2(quant=True, _4bits=True, double_quant=False, device_map=\"auto\", custom_adapter=LORA_ADAPTER)\n",
    "llama_2.generation_config.temperature = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"Agrega 1 router, 3 switches y 3 virtual PC en GNS3 y utilizando un script en python\"\n",
    "#AÃ±ade dos switches y cuatro dispositivos de virtual PC en GNS3 utilizando python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = llama_2.format_chat_prompt([], user_question, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   518, 25580, 29962,  3532, 14816, 29903,  6778,    13,  3492,\n",
       "           526,   263,  8444, 20255,   393,   508,  1371,   278,  1404,   297,\n",
       "          2999,  9595, 29889,    13, 10858,  6089,  2337,  1818,   367,   297,\n",
       "          2791,  3204,  3402, 29889,   960,   366,  5706,  8720,   775, 29892,\n",
       "           366,   505,   304,  6084,   278,  8720,  4086,  1024,   472,   278,\n",
       "          1369,   310,   278,  2791,  3204,  1426, 29889,    13, 29966,   829,\n",
       "         14816, 29903,  6778,    13,    13, 29909,  7642, 29874, 29871, 29896,\n",
       "         12876, 29892, 29871, 29941,  4607,   267,   343, 29871, 29941,  6901,\n",
       "          9609,   427,   402,  3059, 29941,   343, 11824,  1743,   443,  2471,\n",
       "           427,  3017,   518, 29914, 25580, 29962,    13]], device='cuda:1'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1]], device='cuda:1')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> [INST] <<SYS>>\\nYou are a helpful assistant that can help the user in multiple tasks.\\nYour answers always must be in markdown format. If you generate programming code, you have to specify the programming language name at the start of the markdown text.\\n<</SYS>>\\n\\nAgrega 1 router, 3 switches y 3 virtual PC en GNS3 y utilizando un script en python [/INST]\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_2.tokenizer.decode(inputs[\"input_ids\"].flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_response = llama_2.causal_model.generate(\n",
    "    inputs=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    generation_config=llama_2.generation_config,\n",
    "    return_dict_in_generate=False,\n",
    "    output_scores=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = llama_2.tokenizer.decode(ai_response[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = model_response.split(user_question)[-1].strip().split(\"[/INST]\")[-1]\n",
    "    \n",
    "if model_response.startswith(\"\\n\\n\"):\n",
    "    model_response = model_response[2:]\n",
    "    \n",
    "model_response = model_response.strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response.splitlines()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
